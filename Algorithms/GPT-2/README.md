# GPT-2 (Generative Pre-trained Transformer) By OpenAI
This is prediction model based on Transformer class of Natural Language Processing(NLP). Currently the latest version of GPT is GPT-3 released in last quarter of 2020.

This model consists of majorly 3 components:
    
    - Self-attention blocks
        Helps to identify the key points of focus in a given sentence
    - Feedforward Neural Nets
        They consist of further 3 parts as usual:
            - Input layer
            - Hidden layers (relu and swish)
            - Output Layer (softmax)
    - Normalization layer
        exists between last Self-attention block and input layer of feedforward neural net.

        

## References:
    https://rowlando13.medium.com/everything-gpt-2-0-intro-a82e1dd040ae