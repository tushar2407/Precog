# Attention model
> Here are the best references to understand the working behind attention models right from RNN to LSTM to GRU to Seq2Seq to Attention and finally self-attention.

> ## Disadvantages of Attention:
>> One disadvantage is that the context vector is calucated through hidden state between source and target sentence, leaving the attention inside source sentence and target sentence itself ignored 
>
>> Another is that RNN is hard to parallelize, leading the calculation time-consuming 


> ## References
>
    [ ] https://medium.com/@bgg/seq2seq-pay-attention-to-self-attention-part-1-d332e85e9aad
    [ ] https://medium.com/@bgg/seq2seq-pay-attention-to-self-attention-part-2-cf81bf32c73d
    [ ] Coursera course of Andrew Ng - Sequence models

